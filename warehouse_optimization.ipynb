{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation and Imports"
      ],
      "metadata": {
        "id": "DRxX3wP3bJF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (if any)\n",
        "# !pip install pandas\n",
        "\n",
        "# Import libraries\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import sys"
      ],
      "metadata": {
        "id": "OW9pm-aRbNHu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configuration Constants"
      ],
      "metadata": {
        "id": "_E55speXbYR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File configuration\n",
        "INPUT_FILE = \"Mov preparacion 02.2025.csv\"\n",
        "OUTPUT_FILE = \"optimized_warehouse_locations.csv\"\n",
        "\n",
        "# Warehouse configuration\n",
        "LOCATION_FILTER_SUFFIX = \"10\"  # Only process locations ending with this\n",
        "EXCLUDED_LOCATION_PREFIX = \"4\"  # Exclude locations starting with this\n",
        "\n",
        "# Brand conflict configuration\n",
        "BRANDS_TO_SEPARATE = [\"GRANINI\", \"JUVER\", \"MINUTE MAID\"]\n",
        "CONFLICT_DISTANCE = 2  # Minimum distance required between competing brands\n",
        "\n",
        "# Zone sorting configuration (order of processing)\n",
        "ZONE_SORTING_RULES = [\n",
        "    {\"prefix\": \"600\", \"max_zone\": 48, \"condition\": \"<=\"},\n",
        "    {\"prefix\": \"620\", \"max_zone\": None, \"condition\": None},\n",
        "    {\"prefix\": \"660\", \"max_zone\": 43, \"condition\": \"<=\"},\n",
        "    {\"prefix\": \"680\", \"max_zone\": 43, \"condition\": \"<=\"},\n",
        "    {\"prefix\": \"600\", \"max_zone\": 48, \"condition\": \">\"},\n",
        "    {\"prefix\": \"630\", \"max_zone\": None, \"condition\": None},\n",
        "    {\"prefix\": \"660\", \"max_zone\": 43, \"condition\": \">\"},\n",
        "    {\"prefix\": \"680\", \"max_zone\": 43, \"condition\": \">\"},\n",
        "    {\"prefix\": \"690\", \"max_zone\": None, \"condition\": None}\n",
        "]"
      ],
      "metadata": {
        "id": "HNT7udhgbbNI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Loading Functions"
      ],
      "metadata": {
        "id": "X_kg9e4nbizR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_validate_data(file_path):\n",
        "    \"\"\"\n",
        "    Load CSV data with comprehensive error handling and validation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "    # Clean column names (remove leading/trailing spaces)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # Validate required columns exist\n",
        "    required_columns = [\"Material\", \"CtdTeóricaDesde\", \"Texto breve de material\", \"Ubic.proc.\"]\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"Error: Missing required columns: {missing_columns}\")\n",
        "        print(f\"   Available columns: {list(df.columns)}\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "def apply_initial_filters(df):\n",
        "    \"\"\"\n",
        "    Apply business rule filters to the dataset.\n",
        "    \"\"\"\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Filter out locations starting with excluded prefix\n",
        "    df = df[~df[\"Ubic.proc.\"].astype(str).str.startswith(EXCLUDED_LOCATION_PREFIX)]\n",
        "\n",
        "    # Filter for locations ending with specified suffix\n",
        "    df = df[df[\"Ubic.proc.\"].astype(str).str.endswith(LOCATION_FILTER_SUFFIX)]\n",
        "\n",
        "    filtered_count = initial_count - len(df)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "xNre_aKLboJI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Data Processing Functions"
      ],
      "metadata": {
        "id": "ixxcV7Qab8A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_product_groups(df):\n",
        "    \"\"\"\n",
        "    Group products by material and calculate aggregate statistics.\n",
        "    \"\"\"\n",
        "    # Ensure quantity column is numeric\n",
        "    df[\"CtdTeóricaDesde\"] = pd.to_numeric(df[\"CtdTeóricaDesde\"], errors=\"coerce\")\n",
        "\n",
        "    # Calculate total unique products\n",
        "    total_products = df[\"Material\"].nunique()\n",
        "\n",
        "    # Create base data tuples (material, quantity, description, location)\n",
        "    matrix_data = list(zip(\n",
        "        df[\"Material\"],\n",
        "        df[\"CtdTeóricaDesde\"],\n",
        "        df[\"Texto breve de material\"],\n",
        "        df[\"Ubic.proc.\"]\n",
        "    ))\n",
        "\n",
        "    # Group by material using defaultdict for efficient aggregation\n",
        "    grouped_data = defaultdict(lambda: [0, 0, \"\", \"\"])  # [total_qty, count, desc, loc]\n",
        "\n",
        "    for material, quantity, description, location in matrix_data:\n",
        "        if pd.notnull(quantity):\n",
        "            grouped_data[material][0] += quantity  # Sum quantities\n",
        "            grouped_data[material][1] += 1         # Count occurrences\n",
        "            grouped_data[material][2] = description  # Store description\n",
        "            grouped_data[material][3] = location     # Store location\n",
        "\n",
        "    # Convert to list with calculated frequency (count/total_products)\n",
        "    grouped_matrix = [\n",
        "        (material, total, count / total_products, desc, loc)\n",
        "        for material, (total, count, desc, loc) in grouped_data.items()\n",
        "    ]\n",
        "\n",
        "    # Sort by frequency (descending) and then by quantity (descending)\n",
        "    grouped_matrix.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
        "\n",
        "    # Extract original locations for sorting\n",
        "    extracted_locations = [loc for _, _, _, _, loc in grouped_matrix]\n",
        "\n",
        "    return grouped_matrix, total_products, extracted_locations\n",
        "\n",
        "def parse_location(location):\n",
        "    \"\"\"\n",
        "    Parse location string into components.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        block, zone, subzone = location.split(\"-\")\n",
        "        return block, int(zone), subzone\n",
        "    except (ValueError, IndexError):\n",
        "        return \"999\", 999, \"999\""
      ],
      "metadata": {
        "id": "paLZOK9ib-FT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Location Sorting Functions"
      ],
      "metadata": {
        "id": "3anT0XVjcF1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_and_sort_locations(location_list, prefix, max_value=None, condition=None):\n",
        "    \"\"\"\n",
        "    Filter locations by prefix and zone conditions, then sort by zone.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "\n",
        "    for location in location_list:\n",
        "        if location.startswith(prefix):\n",
        "            block, zone, subzone = parse_location(location)\n",
        "\n",
        "            # Apply zone condition if specified\n",
        "            if condition == '<=' and max_value is not None:\n",
        "                if zone <= max_value:\n",
        "                    result.append(location)\n",
        "            elif condition == '>' and max_value is not None:\n",
        "                if zone > max_value:\n",
        "                    result.append(location)\n",
        "            elif condition is None:\n",
        "                result.append(location)\n",
        "\n",
        "    # Sort by zone number\n",
        "    return sorted(result, key=lambda x: parse_location(x)[1])\n",
        "\n",
        "def apply_location_sorting(extracted_locations):\n",
        "    \"\"\"\n",
        "    Apply custom sorting rules to locations based on warehouse zoning.\n",
        "    \"\"\"\n",
        "    sorted_locations = []\n",
        "\n",
        "    # Apply each sorting rule in configuration order\n",
        "    for rule in ZONE_SORTING_RULES:\n",
        "        filtered_locations = filter_and_sort_locations(\n",
        "            extracted_locations,\n",
        "            rule[\"prefix\"],\n",
        "            rule[\"max_zone\"],\n",
        "            rule[\"condition\"]\n",
        "        )\n",
        "        sorted_locations.extend(filtered_locations)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_sorted_locations = list(dict.fromkeys(sorted_locations))\n",
        "\n",
        "    return unique_sorted_locations"
      ],
      "metadata": {
        "id": "mPbog4VzcJkI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conflict Resolution Functions"
      ],
      "metadata": {
        "id": "xle6ndkEcZp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_brand(description, brands):\n",
        "    \"\"\"\n",
        "    Check if product description contains any of the specified brands.\n",
        "    \"\"\"\n",
        "    if pd.isna(description):\n",
        "        return False\n",
        "\n",
        "    description_upper = str(description).upper()\n",
        "    return any(brand.upper() in description_upper for brand in brands)\n",
        "\n",
        "def detect_brand_conflicts(final_products):\n",
        "    \"\"\"\n",
        "    Detect conflicts where competing brands are too close to each other.\n",
        "    \"\"\"\n",
        "    conflicts = []\n",
        "\n",
        "    for i in range(len(final_products)):\n",
        "        current_material, _, _, current_description, current_location = final_products[i]\n",
        "\n",
        "        # Skip if not a brand of interest\n",
        "        if not contains_brand(current_description, BRANDS_TO_SEPARATE):\n",
        "            continue\n",
        "\n",
        "        current_block, current_zone, _ = parse_location(current_location)\n",
        "\n",
        "        # Check neighbors within conflict distance\n",
        "        for offset in [-2, -1, 1, 2]:\n",
        "            j = i + offset\n",
        "            if 0 <= j < len(final_products):\n",
        "                neighbor_material, _, _, neighbor_description, neighbor_location = final_products[j]\n",
        "                neighbor_block, neighbor_zone, _ = parse_location(neighbor_location)\n",
        "\n",
        "                # Check if this is a conflict\n",
        "                if (current_block == neighbor_block and\n",
        "                    abs(current_zone - neighbor_zone) <= CONFLICT_DISTANCE and\n",
        "                    contains_brand(neighbor_description, BRANDS_TO_SEPARATE)):\n",
        "\n",
        "                    # Register conflict (store as sorted tuple to avoid duplicates)\n",
        "                    conflict_pair = tuple(sorted((i, j)))\n",
        "                    conflicts.append(conflict_pair)\n",
        "\n",
        "    # Remove duplicate conflicts\n",
        "    unique_conflicts = list(set(conflicts))\n",
        "\n",
        "    if unique_conflicts:\n",
        "        print(f\"Detected {len(unique_conflicts)} brand conflicts requiring resolution\")\n",
        "    else:\n",
        "        print(\"No brand conflicts detected\")\n",
        "\n",
        "    return unique_conflicts\n",
        "\n",
        "def resolve_brand_conflicts(final_products, conflicts):\n",
        "    \"\"\"\n",
        "    Resolve brand conflicts by swapping locations with non-competing products.\n",
        "    \"\"\"\n",
        "    resolved_count = 0\n",
        "\n",
        "    for i, j in conflicts:\n",
        "        conflict_resolved = False\n",
        "\n",
        "        # Try to resolve by swapping with neighbors of first conflict product\n",
        "        for swap_offset in [-1, 1]:\n",
        "            swap_index = i + swap_offset\n",
        "            if (0 <= swap_index < len(final_products) and\n",
        "                not contains_brand(final_products[swap_index][3], BRANDS_TO_SEPARATE)):\n",
        "\n",
        "                # Swap locations only\n",
        "                final_products[i][4], final_products[swap_index][4] = final_products[swap_index][4], final_products[i][4]\n",
        "                conflict_resolved = True\n",
        "                resolved_count += 1\n",
        "                break\n",
        "\n",
        "        if not conflict_resolved:\n",
        "            # Try to resolve by swapping with neighbors of second conflict product\n",
        "            for swap_offset in [-1, 1]:\n",
        "                swap_index = j + swap_offset\n",
        "                if (0 <= swap_index < len(final_products) and\n",
        "                    not contains_brand(final_products[swap_index][3], BRANDS_TO_SEPARATE)):\n",
        "\n",
        "                    # Swap locations only\n",
        "                    final_products[j][4], final_products[swap_index][4] = final_products[swap_index][4], final_products[j][4]\n",
        "                    resolved_count += 1\n",
        "                    break\n",
        "\n",
        "    if resolved_count > 0:\n",
        "        print(f\"Resolved {resolved_count} brand conflicts through location swapping\")\n",
        "\n",
        "    return final_products"
      ],
      "metadata": {
        "id": "vMZ7b2tGcb6o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Output Functions"
      ],
      "metadata": {
        "id": "FishS5JpcsJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(final_products, total_products):\n",
        "    \"\"\"\n",
        "    Display formatted results of the warehouse optimization.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"WAREHOUSE LOCATION OPTIMIZATION RESULTS\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"Total different products: {total_products}\\n\")\n",
        "\n",
        "    # Table header\n",
        "    print(\"{:<12} {:<12} {:<15} {:<30} {:<15}\".format(\n",
        "        \"Material\", \"Quantity\", \"Frequency\", \"Description\", \"Location\"))\n",
        "    print(\"-\" * 85)\n",
        "\n",
        "    # Display first 20 rows to avoid overwhelming output\n",
        "    display_count = min(20, len(final_products))\n",
        "    for i, (material, total, freq, desc, location) in enumerate(final_products[:display_count]):\n",
        "        print(\"{:<12} {:<12.1f} {:<15.4f} {:<30} {:<15}\".format(\n",
        "            material, total, freq, desc[:28] + \"...\" if len(desc) > 28 else desc, location))\n",
        "\n",
        "    if len(final_products) > display_count:\n",
        "        print(f\"... and {len(final_products) - display_count} more products\")\n",
        "\n",
        "    print(\"-\" * 85)\n",
        "    print(\"Optimization completed successfully\")\n",
        "\n",
        "def save_results_to_csv(final_products, output_file):\n",
        "    \"\"\"\n",
        "    Save optimization results to CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result_df = pd.DataFrame(\n",
        "            final_products,\n",
        "            columns=[\"Material\", \"Quantity\", \"Frequency\", \"Description\", \"Location\"]\n",
        "        )\n",
        "        result_df.to_csv(output_file, index=False)\n",
        "        print(f\"Results saved to '{output_file}'\")\n",
        "\n",
        "        # Show download link in Colab\n",
        "        from google.colab import files\n",
        "        files.download(output_file)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")"
      ],
      "metadata": {
        "id": "7eQV5j58cvIJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Main Execution"
      ],
      "metadata": {
        "id": "OuvfRZvldBPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🎯 WAREHOUSE OPTIMIZATION EXECUTION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Step 1: Upload file first\n",
        "print(\"Please upload your CSV file when prompted\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "if uploaded:\n",
        "    INPUT_FILE = list(uploaded.keys())[0]\n",
        "    print(f\"File '{INPUT_FILE}' uploaded successfully!\")\n",
        "else:\n",
        "    print(\"No file uploaded. Using default filename.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "LlsAeb27dFqA",
        "outputId": "5853a200-dac2-4259-d999-990135242129"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 WAREHOUSE OPTIMIZATION EXECUTION\n",
            "==================================================\n",
            "Please upload your CSV file when prompted\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d964e34a-f87a-4b35-b3fe-ce2886934850\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d964e34a-f87a-4b35-b3fe-ce2886934850\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Mov preparacion 02.2025.csv to Mov preparacion 02.2025 (1).csv\n",
            "File 'Mov preparacion 02.2025 (1).csv' uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Execute Data Processing"
      ],
      "metadata": {
        "id": "OmwiOLfwdf6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and validate data\n",
        "df = load_and_validate_data(INPUT_FILE)\n",
        "if df is None:\n",
        "    print(\"Cannot continue without valid data\")\n",
        "else:\n",
        "    # Apply initial filters\n",
        "    df = apply_initial_filters(df)\n",
        "\n",
        "    # Process product groups\n",
        "    grouped_matrix, total_products, extracted_locations = process_product_groups(df)"
      ],
      "metadata": {
        "id": "D78HILlsdk6g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Execute Location Assignment"
      ],
      "metadata": {
        "id": "qVdfuVGLduF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'grouped_matrix' in locals() and 'extracted_locations' in locals():\n",
        "    # Apply location sorting\n",
        "    sorted_locations = apply_location_sorting(extracted_locations)\n",
        "\n",
        "    # Assign sorted locations to products\n",
        "    final_products = []\n",
        "    for (material, total, frequency, desc, _), new_location in zip(grouped_matrix, sorted_locations):\n",
        "        final_products.append([material, total, frequency, desc, new_location])\n",
        "\n",
        "    print(f\"Assigned locations to {len(final_products)} products\")\n",
        "else:\n",
        "    print(\"Required variables not found. Please run previous blocks first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ghKT5xWdwcp",
        "outputId": "767c6f80-cce2-43e9-f61a-3fcbb1813b04"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigned locations to 370 products\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Execute Conflict Resolution"
      ],
      "metadata": {
        "id": "Q-sXSQBGd1lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'final_products' in locals():\n",
        "    # Detect and resolve brand conflicts\n",
        "    conflicts = detect_brand_conflicts(final_products)\n",
        "    if conflicts:\n",
        "        final_products = resolve_brand_conflicts(final_products, conflicts)\n",
        "else:\n",
        "    print(\"Final products not found. Please run previous blocks first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AaIYcQCd4_u",
        "outputId": "36182d77-6e44-49ea-b623-6537a356f059"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 3 brand conflicts requiring resolution\n",
            "Resolved 3 brand conflicts through location swapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Final Results and Download"
      ],
      "metadata": {
        "id": "aOjjntayeBkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'final_products' in locals() and 'total_products' in locals():\n",
        "    # Display results\n",
        "    display_results(final_products, total_products)\n",
        "\n",
        "    # Save to CSV and download\n",
        "    save_results_to_csv(final_products, OUTPUT_FILE)\n",
        "else:\n",
        "    print(\"Required variables not found. Please run previous blocks first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "A0vmaUSteIhD",
        "outputId": "11162e52-3f59-499a-a8ff-afc276ad5c74"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "WAREHOUSE LOCATION OPTIMIZATION RESULTS\n",
            "==========================================================================================\n",
            "Total different products: 381\n",
            "\n",
            "Material     Quantity     Frequency       Description                    Location       \n",
            "-------------------------------------------------------------------------------------\n",
            "0RF0161      9475.0       1.4961          COCACOLA LATA33 C24 (SIN HC8... 600-004-10     \n",
            "0LT0235      1692.0       1.4803          LA LEVANTINA AVENA ESP.HOST ... 600-006-10     \n",
            "0RF0187      10777.0      1.4593          COCACOLA ZER LATA33 C24 (SIN... 600-007-10     \n",
            "0LT0090      1148.0       1.2756          LETONA SEMI SIN LACTOSA 1L P... 600-008-10     \n",
            "ED13LT       1594.0       1.2598          ESTRELLA DAMM 1/3 LATA         600-009-10     \n",
            "EC13P6       929.0        1.1339          DAURA DAMM 1/3 SR PACK CESTA... 600-011-10     \n",
            "0LT0034      2542.0       1.0420          LETONA SEMI ESPEC HOSTELERIA... 600-012-10     \n",
            "0ZU0020      686.0        1.0184          GRANINI MELOCOTON  20CL 24U    600-013-10     \n",
            "0ZU0024      680.0        1.0105          GRANINI PIÑA 20CL 24U          600-017-10     \n",
            "0LT0036      729.0        0.9239          CACAOLAT VIDRIO 20CL SR 24U    600-019-10     \n",
            "FTI15B24     1703.0       0.9213          FEVERTREE INDIAN TONIC 1/5 B... 600-020-10     \n",
            "0LT0412      1763.0       0.9003          CACAOLAT ORIGINAL 27,5CL PET... 600-021-10     \n",
            "0AM3565      1792.0       0.8871          BARGALLO ACEITE GOURMET SOL ... 600-022-10     \n",
            "NTL13LT6     2478.0       0.8714          NESTEA LIMON 1/3 LATA 4U-PAC... 600-023-10     \n",
            "0RF0019      974.0        0.8556          /-COCA COLA LATA 33CL 24U      600-024-10     \n",
            "0VE0524      875.0        0.8451          SEÑORIO DE LIZIA VERDEJO RUE... 600-025-10     \n",
            "0RF0115      824.0        0.8346          AQUARIUS LATA33 C24            600-026-10     \n",
            "0RF0508      720.0        0.8163          FANTA NAR LATA33 24U (SIN HC... 600-027-10     \n",
            "0RF0252      2982.0       0.7612          COCACOLA PET50 C24 (SIN HC4)... 600-028-10     \n",
            "0RF0079      985.0        0.7454          /-COCA COLA ZERO LATA 33CL 2... 600-029-10     \n",
            "... and 350 more products\n",
            "-------------------------------------------------------------------------------------\n",
            "Optimization completed successfully\n",
            "Results saved to 'optimized_warehouse_locations.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1f4627a5-cc0b-4403-80cc-9e67044f6c7d\", \"optimized_warehouse_locations.csv\", 27081)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}